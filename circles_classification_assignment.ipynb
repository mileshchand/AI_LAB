{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d861d5",
   "metadata": {},
   "source": [
    "# Binary Classification with Neural Networks\n",
    "## Circles Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87595077",
   "metadata": {},
   "source": [
    "### 1. Data Retrieval and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96511d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('circles_binary_classification.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b3c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4g5h6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3cdecb",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning & Feature Design\n",
    "\n",
    "The data is clean, so no preprocessing needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) and labels (y)\n",
    "X = df[['X1', 'X2']].values\n",
    "y = df['label'].values\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "print(f\"X tensor shape: {X.shape}\")\n",
    "print(f\"y tensor shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6",
   "metadata": {},
   "source": [
    "### 3. Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6g7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='red', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Circles Dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8",
   "metadata": {},
   "source": [
    "### 4. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6g7h8i9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0",
   "metadata": {},
   "source": [
    "### 5. Device & Dtype Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h8i9j0k1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move data to device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "### 6. Implement Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j0k1l2m3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelV0: 2 → 5 → 1 (no activation)\n",
    "class ModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5)\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.layer_1(x))\n",
    "\n",
    "model_0 = ModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k1l2m3n4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelV1: 2 → 15 → 15 → 1 (no activation)\n",
    "class ModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=15)\n",
    "        self.layer_2 = nn.Linear(in_features=15, out_features=15)\n",
    "        self.layer_3 = nn.Linear(in_features=15, out_features=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "\n",
    "model_1 = ModelV1().to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l2m3n4o5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelV2: 2 → 64 → 64 → 10 → 1 (with ReLU)\n",
    "class ModelV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=64)\n",
    "        self.layer_2 = nn.Linear(in_features=64, out_features=64)\n",
    "        self.layer_3 = nn.Linear(in_features=64, out_features=10)\n",
    "        self.layer_4 = nn.Linear(in_features=10, out_features=1)\n",
    "        self.relu = nn.ReLU()  # ReLU activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add ReLU between layers\n",
    "        return self.layer_4(self.relu(self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))))\n",
    "\n",
    "model_2 = ModelV2().to(device)\n",
    "model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6",
   "metadata": {},
   "source": [
    "### 7. Loss Function, Optimizer & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n4o5p6q7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (BCEWithLogitsLoss has sigmoid built-in)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5p6q7r8",
   "metadata": {},
   "source": [
    "### 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p6q7r8s9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing function\n",
    "def train_and_test_loop(\n",
    "    model: nn.Module,\n",
    "    epochs: int,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    "    loss_fn: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer\n",
    "):\n",
    "    # Lists to store results\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    test_losses = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        ### Training\n",
    "        model.train()\n",
    "\n",
    "        # 1. Forward pass (model outputs raw logits)\n",
    "        y_logits = model(X_train).squeeze()\n",
    "        y_pred = torch.round(torch.sigmoid(y_logits))  # Convert logits to predictions\n",
    "\n",
    "        # 2. Calculate loss and accuracy\n",
    "        loss = loss_fn(y_logits, y_train)\n",
    "        acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "        # 3. Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        ### Testing\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            # 1. Forward pass\n",
    "            test_logits = model(X_test).squeeze()\n",
    "            test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "            \n",
    "            # 2. Calculate loss and accuracy\n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "        # Store results\n",
    "        loss_list.append(loss.item())\n",
    "        acc_list.append(acc)\n",
    "        test_losses.append(test_loss.item())\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        # Print progress every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "\n",
    "    return loss_list, acc_list, test_losses, test_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "### Helper Functions for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r8s9t0u1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Move data to CPU for plotting\n",
    "    X_np = X.cpu().numpy()\n",
    "    y_np = y.cpu().numpy()\n",
    "    \n",
    "    # Create mesh\n",
    "    x_min, x_max = X_np[:, 0].min() - 0.5, X_np[:, 0].max() + 0.5\n",
    "    y_min, y_max = X_np[:, 1].min() - 0.5, X_np[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        Z = model(torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32).to(device))\n",
    "        Z = torch.sigmoid(Z).cpu().numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu')\n",
    "    plt.scatter(X_np[y_np == 0, 0], X_np[y_np == 0, 1], c='blue', alpha=0.6, edgecolors='k')\n",
    "    plt.scatter(X_np[y_np == 1, 0], X_np[y_np == 1, 1], c='red', alpha=0.6, edgecolors='k')\n",
    "\n",
    "# Plot loss curves\n",
    "def plot_loss_curves(train_losses, test_losses):\n",
    "    epochs = range(len(train_losses))\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9t0u1v2",
   "metadata": {},
   "source": [
    "### 9. Training ModelV0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t0u1v2w3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Reset model and optimizer\n",
    "model = ModelV0().to(device)\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)\n",
    "\n",
    "# Show untrained predictions\n",
    "print(\"Untrained predictions:\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train (Untrained)\")\n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test (Untrained)\")\n",
    "plot_decision_boundary(model, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ModelV0\n",
    "print(\"\\nTraining ModelV0 (100 epochs)...\")\n",
    "train_losses, acc_list, test_losses, test_acc = train_and_test_loop(\n",
    "    model=model,\n",
    "    epochs=100,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# Plot decision boundaries after training\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "plot_loss_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2w3x4y5",
   "metadata": {},
   "source": [
    "**Note:** ModelV0 is **underfitting** - it can't learn the circular pattern because it has no activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3x4y5z6",
   "metadata": {},
   "source": [
    "### Training ModelV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x4y5z6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model and optimizer\n",
    "torch.manual_seed(42)\n",
    "model = ModelV1().to(device)\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)\n",
    "\n",
    "# Train ModelV1\n",
    "print(\"Training ModelV1 (1000 epochs)...\")\n",
    "train_losses, acc_list, test_losses, test_acc = train_and_test_loop(\n",
    "    model=model,\n",
    "    epochs=1000,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "plot_loss_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y5z6a7b8",
   "metadata": {},
   "source": [
    "**Note:** ModelV1 still struggles. Even with more layers, without activation functions, it can only create linear boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z6a7b8c9",
   "metadata": {},
   "source": [
    "### Training ModelV2 (with ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset model and optimizer\n",
    "torch.manual_seed(42)\n",
    "model = ModelV2().to(device)\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=0.1)\n",
    "\n",
    "# Train ModelV2\n",
    "print(\"Training ModelV2 (1500 epochs)...\")\n",
    "train_losses, acc_list, test_losses, test_acc = train_and_test_loop(\n",
    "    model=model,\n",
    "    epochs=1500,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer\n",
    ")\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model, X_train, y_train)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model, X_test, y_test)\n",
    "plt.show()\n",
    "\n",
    "plot_loss_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1",
   "metadata": {},
   "source": [
    "**Result:** ModelV2 performs much better! ReLU activation allows it to learn non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "### Extra Credit: Adam vs SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2g3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Adam optimizer\n",
    "torch.manual_seed(42)\n",
    "model_adam = ModelV2().to(device)\n",
    "optimizer_adam = torch.optim.Adam(params=model_adam.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training ModelV2 with Adam (500 epochs)...\")\n",
    "train_losses_adam, acc_list_adam, test_losses_adam, test_acc_adam = train_and_test_loop(\n",
    "    model=model_adam,\n",
    "    epochs=500,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer_adam\n",
    ")\n",
    "\n",
    "# Compare Adam vs SGD\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses[:500], label='SGD Train')\n",
    "plt.plot(test_losses[:500], label='SGD Test')\n",
    "plt.plot(train_losses_adam, label='Adam Train', linestyle='--')\n",
    "plt.plot(test_losses_adam, label='Adam Test', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss: Adam vs SGD')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc_list[:500], label='SGD Train Acc')\n",
    "plt.plot(test_acc[:500], label='SGD Test Acc')\n",
    "plt.plot(acc_list_adam, label='Adam Train Acc', linestyle='--')\n",
    "plt.plot(test_acc_adam, label='Adam Test Acc', linestyle='--')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Accuracy: Adam vs SGD')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4",
   "metadata": {},
   "source": [
    "**Note:** Adam usually converges faster than SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2g3h4i5",
   "metadata": {},
   "source": [
    "## Discussion and Conclusion\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "**1. Why Activation Functions Are Important:**\n",
    "- ModelV0 and ModelV1 (no activation) could only make **straight lines** to separate data\n",
    "- ModelV2 (with ReLU) could make **curved boundaries** to follow the circles\n",
    "- Without activation functions, the model can't learn complex patterns\n",
    "\n",
    "**2. Underfitting:**\n",
    "- Models without activation are **underfitting** - they're too simple\n",
    "- They can't capture the circular pattern in our data\n",
    "- Even adding more layers doesn't help without activation functions\n",
    "\n",
    "**3. Model Performance:**\n",
    "- ModelV0: ~50% accuracy (just guessing)\n",
    "- ModelV1: ~50% accuracy (still just guessing)\n",
    "- ModelV2: ~95-100% accuracy (actually learning!)\n",
    "\n",
    "**4. Optimizer Comparison:**\n",
    "- Adam optimizer learns faster than SGD\n",
    "- Both can reach good results, but Adam needs fewer epochs\n",
    "\n",
    "### Key Takeaway:\n",
    "**Always use activation functions (like ReLU) between layers!** Without them, your neural network can only draw straight lines, no matter how many layers you add."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
